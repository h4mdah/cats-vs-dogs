{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = \"cuda\" #if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_dir = \"/home/uam/HamdahM/training_set\"\n",
    "test_dir = \"/home/uam/HamdahM/test_set\"\n",
    "new_test_dir = \"/home/uam/HamdahM/test_set_new\"\n",
    "train_dir, test_dir, new_test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/home/uam/HamdahM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Set seed\n",
    "random.seed(42) \n",
    "\n",
    "# 1. Get all image paths (* means \"any combination\")\n",
    "image_path_list= glob.glob(f\"{image_path}/*/*/*.jpg\")\n",
    "\n",
    "# 2. Get random image path\n",
    "random_image_path = random.choice(image_path_list)\n",
    "\n",
    "# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\n",
    "image_class = Path(random_image_path).parent.stem\n",
    "\n",
    "# 4. Open image\n",
    "img = Image.open(random_image_path)\n",
    "\n",
    "# 5. Print metadata\n",
    "print(f\"Random image path: {random_image_path}\")\n",
    "print(f\"Image class: {image_class}\")\n",
    "print(f\"Image height: {img.height}\") \n",
    "print(f\"Image width: {img.width}\")\n",
    "img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Turn the image into an array\n",
    "img_as_array = np.asarray(img)\n",
    "\n",
    "# Plot the image with matplotlib\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(img_as_array)\n",
    "plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -> [height, width, color_channels]\")\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "IMAGE_WIDTH=128\n",
    "IMAGE_HEIGHT=128\n",
    "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "\n",
    "# Write transform for image\n",
    "data_transform = transforms.Compose([\n",
    "    # Resize the images to IMAGE_SIZE xIMAGE_SIZE \n",
    "    transforms.Resize(size=IMAGE_SIZE),\n",
    "    # Flip the images randomly on the horizontal\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n",
    "    # Turn the image into a torch.Tensor\n",
    "    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformed_images(image_paths, transform, n=3, seed=42):\n",
    "    random.seed(seed)\n",
    "    random_image_paths = random.sample(image_paths, k=n)\n",
    "    for image_path in random_image_paths:\n",
    "        with Image.open(image_path) as f:\n",
    "            fig, ax = plt.subplots(1, 2)\n",
    "            ax[0].imshow(f) \n",
    "            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n",
    "            ax[0].axis(\"off\")\n",
    "\n",
    "            transformed_image = transform(f).permute(1, 2, 0) \n",
    "            ax[1].imshow(transformed_image) \n",
    "            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n",
    "            ax[1].axis(\"off\")\n",
    "            fig.suptitle(f\"Class: {Path(random_image_path).parent.stem}\", fontsize=16)\n",
    "\n",
    "plot_transformed_images(image_path_list, transform=data_transform, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "# Creating training set\n",
    "train_data = datasets.ImageFolder(root=train_dir, \n",
    "                                  transform=data_transform, \n",
    "                                  target_transform=None) \n",
    "#Creating test set\n",
    "test_data = datasets.ImageFolder(root=test_dir, transform=data_transform)\n",
    "\n",
    "print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_data.classes\n",
    "class_dict = train_data.class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_data[0][0], train_data[0][1]\n",
    "img_permute = img.permute(1, 2, 0)\n",
    "\n",
    "# Print out different shapes (before and after permute)\n",
    "print(f\"Original shape: {img.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Image permute shape: {img_permute.shape} -> [height, width, color_channels]\")\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.title(class_names[label], fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# How many subprocesses will be used for data loading (higher = more)\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "# Turn train and test Datasets into DataLoaders\n",
    "train_dataloader = DataLoader(dataset=train_data, \n",
    "                              batch_size=1, # how many samples per batch?\n",
    "                              num_workers=NUM_WORKERS,\n",
    "                              shuffle=True) # shuffle the data?\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data, \n",
    "                             batch_size=1, \n",
    "                             num_workers=NUM_WORKERS, \n",
    "                             shuffle=False)\n",
    "\n",
    "train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = next(iter(train_dataloader))\n",
    "\n",
    "# Set image size.\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "\n",
    "# Create training transform with TrivialAugment\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.TrivialAugmentWide(),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# Create testing transform (no data augmentation)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "test_data_augmented = datasets.ImageFolder(test_dir, transform=test_transform)\n",
    "\n",
    "train_data_augmented, test_data_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_dataloader_augmented = DataLoader(train_data_augmented, \n",
    "                                        batch_size=BATCH_SIZE, \n",
    "                                        shuffle=True,\n",
    "                                        num_workers=NUM_WORKERS)\n",
    "\n",
    "test_dataloader_augmented = DataLoader(test_data_augmented, \n",
    "                                       batch_size=BATCH_SIZE, \n",
    "                                       shuffle=False, \n",
    "                                       num_workers=NUM_WORKERS)\n",
    "\n",
    "train_dataloader_augmented, test_dataloader_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ImageClassifierv2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layer_1 = nn.Sequential(\n",
    "          nn.Conv2d(3, 64, 3, padding=1),\n",
    "          nn.ReLU(),\n",
    "          nn.BatchNorm2d(64),\n",
    "          nn.MaxPool2d(2))\n",
    "        self.conv_layer_2 = nn.Sequential(\n",
    "          nn.Conv2d(64, 128, 3, padding=1),  # Reduced to 128 from 512\n",
    "          nn.ReLU(),\n",
    "          nn.BatchNorm2d(128),\n",
    "          nn.MaxPool2d(2))\n",
    "        self.conv_layer_3 = nn.Sequential(\n",
    "          nn.Conv2d(128, 256, 3, padding=1),  # New layer for gradual increase\n",
    "          nn.ReLU(),\n",
    "          nn.BatchNorm2d(256),\n",
    "          nn.MaxPool2d(2))\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # Global Avg Pooling\n",
    "        self.classifier = nn.Sequential(\n",
    "          nn.Flatten(),\n",
    "          nn.Dropout(0.5),  # Added dropout\n",
    "          nn.Linear(in_features=256, out_features=2))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_layer_1(x)\n",
    "        x = self.conv_layer_2(x)\n",
    "        x = self.conv_layer_3(x)\n",
    "        x = self.global_avg_pool(x)  # Replace flattening with GAP\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = ImageClassifierv2().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_batch, label_batch = next(iter(train_dataloader_augmented))\n",
    "\n",
    "# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\n",
    "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
    "print(f\"Single image shape: {img_single.shape}\\n\")\n",
    "\n",
    "# 3. Perform a forward pass on a single image\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    pred = model(img_single.to(device))\n",
    "\n",
    "print(f\"Output logits:\\n{pred}\\n\")\n",
    "print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
    "print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
    "print(f\"Actual label:\\n{label_single}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               adv_lr = 1.0):\n",
    "    \n",
    "    # Setup train loss, adverserial loss and train accuracy values\n",
    "    train_adv_loss, train_loss, train_acc = 0, 0, 0\n",
    "    \n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        mu = X.mean(dim=[2, 3], keepdim=True)\n",
    "        var = X.var(dim=[2, 3], keepdim=True)\n",
    "        sig = (var + 1e-5).sqrt()\n",
    "        mu, sig = mu.detach(), sig.detach()\n",
    "        input_normed = (X - mu) / sig\n",
    "        input_normed = input_normed.detach().clone()\n",
    "        # Set learnable style feature and adv optimizer\n",
    "        adv_mu, adv_sig = mu, sig\n",
    "        adv_mu.requires_grad_(True)\n",
    "        adv_sig.requires_grad_(True)\n",
    "        adv_optim = torch.optim.SGD(params=[adv_mu, adv_sig], lr=adv_lr, momentum=0, weight_decay=0)\n",
    "        # Optimize adversarial style feature\n",
    "        adv_optim.zero_grad()\n",
    "        adv_input = input_normed * adv_sig+ adv_mu\n",
    "        adv_output = model(adv_input)\n",
    "        adv_loss = loss_fn(adv_output, y)\n",
    "        train_adv_loss += adv_loss.item()\n",
    "        (- adv_loss).backward()\n",
    "        adv_optim.step()\n",
    "        \n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        adv_input = input_normed * adv_sig + adv_mu\n",
    "        inputs = torch.cat((X, adv_input), dim=0)\n",
    "        gt = torch.cat((y, y), dim=0)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, gt)\n",
    "        train_loss+= loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(adv_output, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(adv_output)\n",
    "\n",
    "    adv_loss = adv_loss / len(dataloader)\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_adv_loss, train_acc, train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "    \n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "            \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1. Take in various parameters required for training and test steps\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 5):\n",
    "    \n",
    "    # 2. Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": [],\n",
    "        \"adv_loss\": []\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        adv_loss, train_acc, train_loss = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "        \n",
    "        # 4. Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f} | \"\n",
    "            f\"adv_loss: {adv_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "        results[\"adv_loss\"].append(adv_loss)\n",
    "\n",
    "    # 6. Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) \n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=1.0)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "# Train model_0 \n",
    "model_results = train(model=model,\n",
    "                      train_dataloader=train_dataloader_augmented,\n",
    "                      test_dataloader=test_dataloader_augmented,\n",
    "                      optimizer=optimizer,\n",
    "                      loss_fn=loss_fn,\n",
    "                      epochs=NUM_EPOCHS)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(results):\n",
    "  \n",
    "    results = dict(list(model_results.items()))\n",
    "\n",
    "    # Get the loss values of the results dictionary (training and test)\n",
    "    loss = results['train_loss']\n",
    "    test_loss = results['test_loss']\n",
    "\n",
    "    # Get the accuracy values of the results dictionary (training and test)\n",
    "    accuracy = results['train_acc']\n",
    "    test_accuracy = results['test_acc']\n",
    "\n",
    "    # Figure out how many epochs there were\n",
    "    epochs = range(len(results['train_loss']))\n",
    "\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='train_loss')\n",
    "    plt.plot(epochs, test_loss, label='test_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
    "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "    plot_loss_curves(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a image.\n",
    "custom_image_path = \"/home/uam/HamdahM/test_set/cats/cat.4017.jpg\"\n",
    "\n",
    "import torchvision\n",
    "# Load in custom image and convert the tensor values to float32\n",
    "custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n",
    "\n",
    "# Divide the image pixel values by 255 to get them between [0, 1]\n",
    "custom_image = custom_image / 255. \n",
    "\n",
    "# Print out image data\n",
    "print(f\"Custom image tensor:\\n{custom_image}\\n\")\n",
    "print(f\"Custom image shape: {custom_image.shape}\\n\")\n",
    "print(f\"Custom image dtype: {custom_image.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_image_transform = transforms.Compose([\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "])\n",
    "\n",
    "# Transform target image\n",
    "custom_image_transformed = custom_image_transform(custom_image)\n",
    "\n",
    "# Print out original shape and new shape\n",
    "print(f\"Original shape: {custom_image.shape}\")\n",
    "print(f\"New shape: {custom_image_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    # Add an extra dimension to image\n",
    "    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n",
    "    \n",
    "    # Print out different shapes\n",
    "    print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n",
    "    print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n",
    "    \n",
    "    # Make a prediction on image with an extra dimension\n",
    "    custom_image_pred = model(custom_image_transformed.unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_image_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Prediction logits: {custom_image_pred}\")\n",
    "\n",
    "custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\n",
    "print(f\"Prediction probabilities: {custom_image_pred_probs}\")\n",
    "\n",
    "custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\n",
    "print(f\"Prediction label: {custom_image_pred_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\n",
    "custom_image_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -> HWC otherwise matplotlib will error\n",
    "plt.title(f\"Image shape: {custom_image.shape}\")\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    # Collect the element-wise sign of the data gradient\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    # Create the perturbed image by adjusting each pixel of the input image\n",
    "    perturbed_image = image + epsilon*sign_data_grad\n",
    "    # Adding clipping to maintain [0,1] range\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    # Return the perturbed image\n",
    "    return perturbed_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "image_path = \"/home/uam/HamdahM/test_set/cats/cat.4017.jpg\"\n",
    "\n",
    "custom_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n",
    "\n",
    "custom_image = custom_image / 255\n",
    "\n",
    "custom_image_transformed = custom_image_transform(custom_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_image_transformed.requires_grad = True\n",
    "output = model(custom_image_transformed.unsqueeze(dim=0).to(device))\n",
    "predicted_label = torch.argmax(output, 1).item()\n",
    "predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = torch.tensor([0]).to(device)\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()(output, true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "loss.backward()\n",
    "data_grad = custom_image_transformed.grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.125\n",
    "perturbed_image = fgsm_attack(custom_image_transformed, epsilon, data_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(perturbed_image.unsqueeze(dim=0).to(device))\n",
    "new_predicted_label = torch.argmax(output, 1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_np = np.transpose(custom_image_transformed.squeeze().detach().numpy(), (1, 2, 0))\n",
    "perturbed_image_np = np.transpose(perturbed_image.squeeze().detach().numpy(), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_label(image, label):\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title(label)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_image_label(image_np, f\"Orignal Label: {predicted_label}\")\n",
    "show_image_label(perturbed_image_np, f\"Adverserial Label: {new_predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 107\u001b[0m\n\u001b[1;32m    105\u001b[0m     plt\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    106\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m--> 107\u001b[0m \u001b[43mt_sne\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_2\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[116], line 86\u001b[0m, in \u001b[0;36mt_sne\u001b[0;34m(dataloader1, dataloader2)\u001b[0m\n\u001b[1;32m     83\u001b[0m f \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m     84\u001b[0m ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplot(aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mequal\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     85\u001b[0m sc \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mscatter(X_tsne[:,\u001b[38;5;241m0\u001b[39m], X_tsne[:,\u001b[38;5;241m1\u001b[39m], lw\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m40\u001b[39m,\n\u001b[0;32m---> 86\u001b[0m                 c\u001b[38;5;241m=\u001b[39mpalette[all_y\u001b[38;5;241m.\u001b[39mastype(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m)])\n\u001b[1;32m     87\u001b[0m ax\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# We add the labels for each digit.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/__init__.py:324\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    319\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtesting\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# So that you won't get the OpenMP related warning\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# Set image size.\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset_1 = datasets.ImageFolder(test_dir, transform=test_transform)  # usual cats and dogs dataset\n",
    "dataset_2 = datasets.ImageFolder(new_test_dir, transform=test_transform)  # the one I gave you with modified images\n",
    "dataloader_1 = DataLoader(dataset_1, batch_size=1, shuffle=False, num_workers=NUM_WORKERS)\n",
    "dataloader_2 = DataLoader(dataset_2, batch_size=1, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "\n",
    "# Assuming you have a DataLoader object named 'dataloader'\n",
    "# Iterate over the DataLoader to get batches of data\n",
    "def t_sne(dataloader1: torch.utils.data.DataLoader, dataloader2: torch.utils.data.DataLoader):\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(dataloader1):\n",
    "        # This time already is numpy as we don't have to_tensor in the transform\n",
    "        X = X_batch.numpy()\n",
    "        # compute the channel-wise mean and standard deviation\n",
    "        mean = X.mean(axis =(0, 2, 3)) \n",
    "        std = X.std(axis =(0, 2, 3))\n",
    "        y = y_batch.numpy()\n",
    "        # add mean and std to one list\n",
    "        mean_std = np.concatenate([mean, std])\n",
    "        all_X.append(mean_std)\n",
    "        all_y.append(y)\n",
    "\n",
    "    for batch_idx, (X_batch, y_batch) in enumerate(dataloader2):\n",
    "        X = X_batch.numpy()\n",
    "        mean = X.mean(axis  =(0, 2, 3))\n",
    "        std = X.std(axis =(0, 2, 3))\n",
    "        y = y_batch.numpy()\n",
    "        # We will add 2 to the labels of the second dataset to distinguish them\n",
    "        y += 2\n",
    "        mean_std = np.concatenate([mean, std])\n",
    "        all_X.append(mean_std)\n",
    "        all_y.append(y)\n",
    "\n",
    "    all_X = np.vstack(all_X)\n",
    "    all_y = np.vstack(all_y)\n",
    "    all_y =all_y.flatten()\n",
    "\n",
    "\n",
    "    # Apply t-SNE for dimensionality reduction\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=20)\n",
    "    X_tsne = tsne.fit_transform(all_X)\n",
    "    # Define colors\n",
    "    colors = [\"blue\", \"grey\", \"red\", \"black\"]\n",
    "\n",
    "    # Plot the t-SNE diagram with custom colors for each class and dataset\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i in range(4):\n",
    "        plt.scatter(X_tsne[all_y == i, 0], X_tsne[all_y == i, 1], color = colors[i], label=f\"Class {i}\", alpha=0.3)\n",
    "    plt.title(\"t-SNE Plot of Two Datasets with Two Classes Each\")\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "t_sne(dataloader_1, dataloader_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
