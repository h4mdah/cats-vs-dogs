{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25920047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerImageProcessor, ResNetForImageClassification\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "# Image processor does some basic image preprocessing and normalizing\n",
    "# usually to match the image size and format the model was trained on\n",
    "image_processor = SegformerImageProcessor.from_pretrained(\"microsoft/resnet-18\",\n",
    "                                                     do_resize=False, \n",
    "                                                     # since you are already resizing the image in your transformations to the appropriate size (224,224)\n",
    "                                                     do_recale=False, \n",
    "                                                     # this is the division by 255 to normalize the pixel values to [0,1], you are already doing this in your transformations\n",
    "                                                     do_normalize=True, # let it be true, here the image gets nnormalized to the mean and std of the imagenet dataset\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35993a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-18 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 512]) in the checkpoint and torch.Size([2, 512]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from pretrained is similar to the load from checkpoint in the previous example\n",
    "# huggingface has a hub which contains all the pretrained models, it is like github for models\n",
    "# so when you provide the model name, it will download the model from the hub if available \n",
    "# and if not, it will download the model from the model zoo and cache it in the hub\n",
    "# it can also used to load the model from a local directory\n",
    "model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-18\", \n",
    "                                                     num_labels=2, # we only have 2 labels\n",
    "                                                     ignore_mismatched_sizes=True, \n",
    "                                                    )\n",
    "                                                     # the model was trained to predict 1000 classes, but we only have 2, so we ignore the mismatched weights\n",
    "# you will see some warnings about the weight mismatch, but you can ignore them                                                     \n",
    "                                                     \n",
    "                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48f63ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===================================================================================================================\n",
       "Layer (type:depth-idx)                                            Output Shape              Param #\n",
       "===================================================================================================================\n",
       "ResNetForImageClassification                                      [1, 2]                    --\n",
       "├─ResNetModel: 1-1                                                [1, 512, 1, 1]            --\n",
       "│    └─ResNetEmbeddings: 2-1                                      [1, 64, 56, 56]           --\n",
       "│    │    └─ResNetConvLayer: 3-1                                  [1, 64, 112, 112]         9,536\n",
       "│    │    └─MaxPool2d: 3-2                                        [1, 64, 56, 56]           --\n",
       "│    └─ResNetEncoder: 2-2                                         [1, 512, 7, 7]            --\n",
       "│    │    └─ModuleList: 3-3                                       --                        11,166,976\n",
       "│    └─AdaptiveAvgPool2d: 2-3                                     [1, 512, 1, 1]            --\n",
       "├─Sequential: 1-2                                                 [1, 2]                    --\n",
       "│    └─Flatten: 2-4                                               [1, 512]                  --\n",
       "│    └─Linear: 2-5                                                [1, 2]                    1,026\n",
       "===================================================================================================================\n",
       "Total params: 11,177,538\n",
       "Trainable params: 11,177,538\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 1.81\n",
       "===================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 39.74\n",
       "Params size (MB): 44.71\n",
       "Estimated Total Size (MB): 85.05\n",
       "==================================================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets see the model in detail\n",
    "import torchinfo\n",
    "torchinfo.summary(model, input_size=[1, 3, 224 ,224]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "944db764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as you can see, the Sequential: 1-2 is similar to the classifier head in the previous example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62b8b3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n",
      "logits: tensor([[-0.1175,  0.5848]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
      "Loss: 1.1047608852386475\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# image is the X in the kaggle code\n",
    "# y is the label of the image, i.e. the class of the image\n",
    "# since we loaded a cat image dataset, the label will be 0\n",
    "\n",
    "y = np.array([[0]]) # this is the label of the image, 0 is the class of the image \n",
    "X  = np.array(image)\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "# you need to preprocess the image before passing it to the model\n",
    "inputs = image_processor(X, y, return_tensors=\"pt\")\n",
    "outputs = model(**inputs.to(device))\n",
    "logits = outputs.logits \n",
    "# logits are unnormalized probabilities, may be not the most accurate definition\n",
    "# so usually probabilities lies between 0-1. Logits are real values (can be negative or positive)\n",
    "# normalizing the logits with softmax function will give you the probabilities\n",
    "\n",
    "loss = outputs.loss\n",
    "# some models in transformers library can compute the loss when you provide the labels (y)\n",
    "# you can call backward on the loss to compute the gradients and update the weights of the model\n",
    "\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=-1).to(device)\n",
    "predicted_class = torch.argmax(probabilities, dim=-1).to(device)\n",
    "# you can also call argmax on logits, will give you the same result\n",
    "\n",
    "print(f\"Predicted class: {predicted_class.item()}\")\n",
    "print(f\"logits: {logits}\")\n",
    "print(f\"Loss: {loss.item()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
